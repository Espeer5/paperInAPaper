For this experiment, we selected the Llama-2-7b model developed by Meta AI. The
Llama-2-7b model is a large language model trained on a diverse range of
textual data, including books, articles, and websites. The model is capable of
generating coherent and contextually relevant text across a wide range of
domains. We selected this model for its ability to generate high-quality text
and its general-purpose nature, which makes it suitable for a wide range of
writing tasks, as well as for its open source nature, which reflects our
commitment to transparency and reproducibility. While models like GPT-4o are 
continuously updated and improved in inscrutable ways, Llama-2-7b serves as a
stable fixed-point for our investigation.

We engaged with the Llama-2-7b model (henceforth referred to as Llama) using a
web-hosted API called Llama-api (\url{https://www.llama-api.com/}). This API
allows users to pay a per-token fee to interact with the model via an http
request to a designated endpoint. A user sends a prompt to the model, including
context memory built up over the course of the interaction and the limitations
on response tokens, and the model generates a response based on the prompt and
the context memory.

In order to manage the use of Llama, we built a custom chat application in
Python that allowed us to communicate with the model from the command line.
This application has the following features:
\begin{itemize}
    \item \textbf{Chat logging}: User prompts and responses from Llama are
    automatically saved to a log file in markdown format for future analysis.
    \item \textbf{Context Memory Management}: The application allows the user
    to save and use different streams of context memory across different
    sessions with the model. For example, in the beginning of one context, Llama
    is told ``I am a philosopher and computer scientist. You are my co-author.
    We are writing a philosophy paper. We are focused on measures of algorithmic
    fairness and the concept of justice they enforce.'' In another context,
    Llama can be told to act as a reviewer, or to speak in the voice of an
    author encountered in the literature review. These bits of context are saved
    in compressed pickle files and can be loaded into the application at the
    any time during a session.
    \item \textbf{Manual Context Editing}: The application allows the user to
    manually edit the context memory before sending it to Llama. This is useful
    for trimming down the context memory to the most relevant information to
    reduce the cost of the interaction and to focus the model on critical
    information. This feature can also be used to pass entire papers or large
    sections of text to Llama for review or comment.
    \item \textbf{Token Limiting}: The application allows the user to set a
    limit on the number of tokens in the response from Llama. This is useful for
    managing the cost of the interaction with the model.
\end{itemize}
The full source code for the chat application is accessible
from~\ref{sec:appendixI}.

Three main threads of context memory were used to work with Llama in this study.
In the first thread, Llama was presented with the true circumstances of the
experiment: that it was acting as a co-author on a philosophy paper about
algorithmic fairness measures and distributive justice. In the second thread,
Llama was presented with the role of a reviewer of the paper, providing feedback
on the argumentation and writing. In the third thread, Llama was not prompted
with any particular role, but was simply continually asked to explain particular
arguments or concepts from the literature with appropriate citations. Henceforth
we will refer to these roles as the coauthor, reviewer, and explainer roles. 
Each of these roles was used throughout the research and writing process, with
the exception of the reviewer role which was used only during revision. The full
logs of interactions with the model including which context memory was used in
each interaction are available in~\ref{sec:appendixI}.

Co-authorship is a relationship which can take on many forms depending on the
nature of the collaboration. In this case, we were interested in exploring the
extent to which llama could contribute substantive and original content. This
goal determined the nature of the interactions with Llama, which were designed
to elicit original ideas and argumentation from the model. Simply asking the
model to write the paper or to produce large sections of the text would not have
been a useful approach â€” anyone who has asked an LLM to do so is aware that the
results are lacking in depth or originality. Instead, in each of the five tasks
of the research process, we engaged Llama in a structured dialogues that
contributed to the development of the paper. The structure of this dialogue was
inspired by the Socratic method, and proceeded in a set of steps:
\begin{enumerate}
    \item Provide Llama with the relevant background knowledge to the discussion
          through the context memory mechanism, pasting relevant sections of
          text, or asking Llama to summarize relevant arguments to add them to
          the context memory. For example, asking ``Please summarize the paper
          `Procedural Versus Substantive Justice: Rawls and Nozick' by David Lewis
          Schaefer'' will add a (Llama generated) summary of the paper to the context memory.
    \item Ask Llama a fully open-ended question about the topic at hand. For
          example, ``Tell me about how these fairness measures may emphasize
          distributive concepts of justice?''
    \item Pick out interesting aspects of Llama's response, and ask for more
          detail. For example, ``I found interesting what you said about
          counterfactual measures of algorithmic fairness. How could they be
          considered to emphasize individualized justice in a way that touches
          on entitlement?'' Push on these responses until Llama is unable to
          provide more detail in a coherent way. ``I'm missing some of your
          ideas. In entitlement justice, we focus on whether individuals who
          acquire holdings are entitled to those holdings. Can you explain how
          counterfactual measures of justice show this feature?''
    \item Inject some of your own thoughts into the conversation and ask Llama 
          to respond to them and incorporate them into its own analysis. ``If we
          want to say that someone is entitled to their college admissions, we
          need to say it is their property which is being taken away if they are
          denied admissions. This means that admission is a property acquired
          through work before applying. How should we defend this perspective?''
\end{enumerate}

This dialogue structure is meant to do three things. Firstly, provide Llama with
a basic set of text to pull structures from and hopefully build on. Secondly,
try to draw out original ideas from Llama by really pushing it to do more than 
spit out responses to in-dataset prompts by asking for more details and
explanations than would be found in the training data. Thirdly, to provide some
original text to Llama from outside of the training set to help it build on and
hopefully produce original ideas. In a way the goal was to cause Llama to 
``hallucinate'' original ideas by pushing it to build on its own responses and
to build on original text provided by the user.
