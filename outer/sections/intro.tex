The majority of research journals now provide policies for the use of large
language models (LLMs) as academic writing. In Nature journals, for example, ``Large Language Models do not
satisfy our authorship criteria. Notably, an attribution of authorship carries 
with it accountability for the work, which cannot be effectively applied to
LLMs.'' In the Cambridge Press, ``AI does not meet the Cambridge requirements
for authorship, given the need for accountability.'' The chief concern among
these policies appears to be responsibility. Who should the journal turn to when
a mistake is discovered in a paper? Who is the owner and originator of the ideas
presented? Who should be legally liable for the content of the paper? The
policies are clear that human authors alone must take full responsibility for the
content of their papers, and to that end, LLMs cannot be considered coauthors.
This is a reasonable provision for ensuring accountability — how would one hold
an LLM accountable for an error in writing? — but it also has the effect that
\emph{should} an LLM produce any original ideas that make it into a published
paper, those ideas will not be attributed to the LLM but to the human authors,
which represents a potential ethical dilemma if we find LLMs capable of
contributing novel ideas and argumentation. This of course raises the question
of whether LLMs are capable of doing so and to what extent.

The common role LLMs hinted at by these policies is as a copy-editor or
proofreader, where the LLM is used to improve the clarity and coherence of
academic writing, but not as an intellectual collaborator in the sense of 
contributing ideas and argumentation to the writing. In this paper, we attempt
to explore the latter role, and to investigate the extent to which a current LLM
can serve as a coathor in the sense of contributing ideas and argumentation to
an original philosophical research paper. We do this by using a LLM to
collaborate on a research paper in the field of algorithmic fairness and
distributive justice. The success of such an endeavor is difficult to measure;
we seek to provide a qualitative assessment of the LLM's contributions to the
paper, and to explore the implications of the growing capabilities of LLMs for
the future of academic writing. With the ongoing and rapid development of LLMs
in mind, this paper is not meant to be a definitive assessment of the
capabilities of LLMs, but rather an exploration of current possibilities and
limitations as well as a discussion of the future of academic writing in light
of these developments.

For this project, we chose as the subject of our paper the topic of algorithmic
fairness measures and their connection to the philosophical concept of
distributive justice. Algorithmic fairness measures are a critical component of
the design and deployment of machine learning systems, as they are intended to
ensure that these systems do not discriminate against individuals based on
sensitive attributes. Distributive justice, on the other hand, is a central
concept in political philosophy that concerns the fair distribution of social
goods. Our investigation began from this vague notion of the intersection
between algorithmic fairness measures and distributive justice, and we used the
LLM as a collaborator to help us refine and develop our ideas from this starting
point.

This paper will be structured as follows. In Section~\ref{sec:methods}, we will
present the methods used, including the specific LLM selected for the
investigation and the program used to interact with it. In
Section~\ref{sec:results}, we will present the full text of the paper produced
in collaboration with the LLM. In \ref{sec:analysis}, we will analyze the role
of the LLM throughout 5 task-stages of the research process: literature review,
research question formulation, argumentation, writing, and revision. Finally,
in Section~\ref{sec:conclusion}, we will conclude with a discussion of the
implications of this experiment for the future of academic writing.
