The majority of research journals now provide policies for the use of large
language models (LLMs) in academic writing. In the Nature journals, for example,
``Large Language Models do not satisfy our authorship criteria. Notably, an
attribution of authorship carries with it accountability for the work, which
cannot be effectively applied to LLMs''~\citep{nature_ai_policies}. For Cambridge
Press, ``AI does not meet the Cambridge requirements for authorship, given the
need for accountability''~\citep{cambridge_ai_policies}. The chief concern among
these policies appears to be responsibility. The policies don't outright ban the
use of AI, nor do the outline specific guidelines for how to appropriately use
LLMs, save for as a copy-editor, but they make specific prohibitions against
authorship and attribution of writing to LLMs. It seems clear that the journals
are targeting issues of accountability. Who should the journal turn to when a
mistake is discovered in a paper? Who is the owner and originator of the ideas
presented? Who should be legally liable for the content of the paper? 

The policies are clear that human authors alone must take
full responsibility for the content of their papers, and to that end, LLMs
cannot be considered coauthors. The need for accountability goes beyond concerns
about liability for errors in writing, however, The policies also reflect a
concern about the ownership of ideas and plagiarism. LLMs are trained on vast 
amounts of text, and it is not always clear how to attribute information they
produce. An LLM may produce information that is similar or identical to text in
its training data without a citation, and if this text is included in a paper,
it constitutes plagiarism. The emphasis on accountability in these policies
implies that in this situation, the human authors of the paper would be held
liable for the plagiarism, even if they were unaware of it.

The policies protect the journals from liability in the case of these plagiarism
concerns, but they are largely predicated on the idea that LLMs are not capable
of originating novel ideas of their own. The policies are clear that LLMs cannot
be attributed authorship, meaning that if they did produce original arguments,
authors would be faced with a choice of either not publishing those ideas 
(stifling potentially significant contributions to the field) or not properly
attributing them, neither of which is a desirable outcome. This means that
should we find LLMs capable of producing original ideas, the current policies
are out of step with the capabilities of the technology and require rapid 
revision. Even if LLMs do not currently have this ability, it is possible they
could attain them in the near future, and thus these policies are at risk of
quickly becoming out of date. We therefore need to both discover and explore the
capabilities of LLMs in producing new ideas, and to consider the implications
of these capabilities for the future of academic writing.

In this paper, we attempt to explore the ability of a current LLM to contribute
original ideas and argumentation to an original philosophical research paper.
We do this by using an LLM to collaborate on a research paper in the field of
algorithmic fairness and distributive justice. The goal of the project is to
produce a high-quality research paper that constitutes an original contribution
to the field, and in the process, to use the LLM to its fullest potential and
explore the capabilities and limitations of doing so. The goal of this
experiment is not simply to see if the LLM can produce a publishable paper, but
rather to explore the utility of using an LLM in a genuine effort to produce a
high-quality paper. This effort will be a collaborative one, with both the human
author and LLM writing sections of the paper, providing feedback on each other's
writing, suggesting sources, and engaging in discussion about the ideas
presented with the goal of producing the best paper possible. The success of
such an endeavor is difficult to measure; we seek to provide a qualitative
assessment of the LLM's contributions to the paper, and to explore the
implications of the growing capabilities of LLMs for the future of academic
writing. With the ongoing and rapid development of LLMs in mind, this paper is
not meant to be a definitive assessment of the capabilities of LLMs, but rather
an exploration of a specific model's capabilities and limitations as well as a
discussion of the future of academic writing in light of these technologies.

For this project, we chose as the subject of our paper the topic of algorithmic
fairness measures and their connection to the philosophical concept of
distributive justice. Algorithmic fairness measures are a critical component of
the design and deployment of machine learning systems, as they are intended to
ensure that these systems do not discriminate against individuals based on
sensitive attributes. Distributive justice, on the other hand, is a central
concept in political philosophy that concerns the fair distribution of social
goods. Distributive justice exists on a spectrum from end-state theories, which
analyze whether a given distribution of resources is fair, to historical
theories, which analyze whether the history of transactions that led to a given
distribution of resources is fair. Since algorithmic fairness measures are often
used to evaluate the fairness of algorithmic systems that make decisions about
the distribution of resources such as bank loans or job opportunities, there is
a natural connection between these two topics. The relationship between the two
fields has been explored in the literature, but in the early stages of this
project, our LLM suggested that there is a lack of discussion in the literature
about the relationship between algorithmic fairness measures and historical
theories of distributive justice. Our investigation began from this vague notion
of the intersection between the two fields, and we worked with the LLM from this
point to develop and complete a specific research project on the topic.

This paper will be structured as follows. In Section~\ref{sec:methods}, we will
present the methods used, including the specific LLM selected for the
investigation and the program used to interact with it. In
Section~\ref{sec:results}, we will present the full text of the paper produced
in collaboration with the LLM. In Section~\ref{sec:analysis}, we will analyze
the role of the LLM throughout 5 task-stages of the research process: literature
review, research question formulation, argumentation, writing, and revision.
Finally, in Section~\ref{sec:conclusion}, we will conclude with a discussion of
the implications of this experiment for the future of academic writing. Note
that an LLM was only used as a collaborator for the writing of the research
paper presented in Section~\ref{sec:results}, and not for the writing of this
introduction or any other part of the paper. The introduction, analysis, and
discussion outside of Section~\ref{sec:results} were written entirely by the
human author of this paper without the assistance of LLMs.
