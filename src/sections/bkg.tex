In this paper, we will center our attention on a class of decision problems
corresponding to the following formalism. There exists some population of 
individuals $I = \{i_1, i_2, \ldots, i_n\}$ over whom we must distribute some
finitely divisible amount of a resource, $R$. A \textit{decision rule} is a
mapping $d: I \rightarrow \{0, 1\}$, under which $i_n$ receives $R$ iff
$d(i_n) = 1$. As an example, consider the process of allocating loans over a
pool of applicants — for each individual in the applicant pool, we have a binary
choice to either approve or deny the loan, and only on acceptance does the
individual receive capital.

An algorithmic decision maker in this setup is a system, particularly a
technical system, under which a decision rule is implemented as a set of steps
which are applied identically to all individuals. This broadly consists of two
tasks. First, a set of covariates $X = \{x_1, x_2, \ldots x_n\}$ must be\
collected for each individual. Then, a classification scheme
$f: X \rightarrow \{0, 1\}$ must be used to map each individual to an outcome.
Returning to the loan case, a simple example would be the following: Approve a
loan to each individual whose household income exceeds the projected cost of
living in the geographic location of their residence by at least \$10,000 per
year. Then our algorithm implementing the decision rule for each applicant takes
the following shape:
\begin{itemize}
    \item Collect covariates $X=\{$household income, geographic location of residence$\}$
    \item Let $C(x_2) =$ projected cost of living in location of residence, $x_1 =$
          household income
    \item $f(X) = x_2 - C(x_1) \geq 10000$
\end{itemize}

It is important to note that not all problem domains to which algorithmic
decision making is applied can be formulated in this way. For example,
applications of AI in natural language translation may not be easily formulated
in terms of resource allocation, but the reader may still be concerned with the
perpetuation of social biases through the decisions made in translation — for
example, issues of underrepresentation of social groups in translated media.  
While these cases warrant further study, we will not consider them here.

\subsection{Algorithmic Fairness Measures}\label{sec:fairness-measures}

Algorithmic fairness measures as they're presented in the literature operate on
the classification scheme $f$ and a set of morally protected characteristics
such as race or gender $A \subseteq X$, attempting to enforce constraints on how 
decisions can be sensitive to such characteristics. An overview of measures
commonly discussed in the literature is presented below.

A critical point to note in the discussion of algorithmic fairness is the 
distinction between individual and group fairness. In group fairness, we form
groups of individuals based on the protected attributes $A$, and measure
fairness as a statistical property on the distribution of outcomes across these
groups. For example, we may measure the proportion of individuals from each
group who received a positive outcome from the classification function. In
individual fairness, on the other hand, we attempt to judge whether like
individuals are being treated alike by the algorithm. For example, two
individuals who are identical but for their race should receive the same
classification outcome in decision-making domains where race is not relevant. 

Both group fairness and individual fairness measures have conceptual
shortcomings. The coarse-grained nature of group fairness measures means that
there is no guarantee that individuals within a group are treated fairly, only
that the algorithm is statistically unbiased in the aggregate, discarding
the potential for bias against particular individuals in the group. Individual
fairness measures, on the other hand, require a notion of similarity between
individuals which is often difficult to define. Never are two individuals
truly identical but for their race, and the choice of a similarity metric can
encode normative assumptions about the importance of different attributes in
determining the outcome of the classification function.

To set the scene for further discussion of how algorithmic fairness is 
connected to distributive justice, a survey of common algorithmic fairness 
measures together with their strengths and weaknesses is presented below.
We will discuss these measures in the context of the decision problem formulated
above. The goal of this discussion is to highlight the limitations of
existing algorithmic fairness measures in capturing the normative concerns of 
entitlement justice. For a more exhaustive presentation of existing measures,
see~\cite{CorbettDavies_2023}.

As a running example, consider the case of a hiring algorithm. The decision
problem is to map a set of applicants to hiring decision. Our algorithm must
implement a function to do so based only on the personal information presented
in their resumes, which is their education, experience, and cover letter. We
want to ensure that our hiring practices are fair with respect to race, and
gender. 

\begin{definition}
    Fairness Through Unawareness — $f$ satisfies fairness through unawareness iff
    \[A = \emptyset\]
\end{definition}

In other words, the classification function may not receive any morally
protected covariates as inputs. So, for our hiring admissions case, we would be
forced to remove race and gender from $X$ and only consider education and
experience.

This notion is intuitively appealing — how can the algorithm discriminate
against one based on their race if it doesn't know their race? However, it is
clear that this measure is not sufficient to ensure fairness. For example, if
one attended a historically black college, their education may function as a
proxy for their race, allowing bias to remain in the algorithm.

\begin{definition}
    Demographic Parity — $f$ satisfies demographic parity iff
    \[P[f(X) = 1 | A = a] = P[f(X) = 1]\;\forall a \in A\]~\citep{Dwork_2012}.
\end{definition}

Demographic parity holds that the probability of a positive output ($f(X) = 1$)
should be statistically independent of the protected attributes. This is 
an easily understandable and measurable criteria for fairness. At a first look,
it is appealing — the same number of individuals from each race will be
successful in seeking jobs at a particular company. Indeed, in a situation like
selection of individuals for a representative committee this measure is
appropriate. However in other circumstances such as hiring or parole,
difficulties arise.

Under demographic parity, the probability of success between
groups must be balanced, hut if the base rates of success are unequal between
groups, this leads to poor outcomes. For example, if women are much more
qualified for a job on average than men, then demographic parity will require
that we hire less qualified men in place of more qualified women in order to
balance the probability of being selected between gender groups. In other words,
the false positive rate will be very high for men while the false negative rate
will be very high for women, creating a severely unfair
practice~\citep{Barocas_2017}.

\begin{definition}
    Equalized Odds — $f$ satisfies equalized odds if given a true outcome $o$
    for each individuals, we have
    \[P[f(X)=1|A=a, o=1] = P[f(X)=1|A=b, o=1]\;\forall a, b\in A\]
    \[P[f(X)=1|A=a, o=0] = P[f(X)=1|A=b, o=0]\;\forall a, b\in A\]~\citep{Hardt_2016}.
\end{definition}

In cases where the output value of the classifier $f$ disagrees with the true
outcome $o$, we have either a false position or false negative. 
Equalized odds requires that the true positive and false positive rates be
balanced between groups. This is often thought to ensure there is no disparate 
mistreatment across groups. In our hiring case, for example, equalized odds
ensures that no one racial group is more likely to be falsely rejected or
erroneously hired than another. Given one has been rejected, the probability it
was a wrongful rejection is equal regardless of their race or gender.

Equalized odds has two concerns associated with it. Firstly, the condition
becomes difficult to satisfy when the base rates of success are unequal between
groups, as differing base rates imply differing false positive and false
negative rates. In our hiring case, once again imagine that women are much more
qualified for a job on average than men. This condition in the population
would lead to a model distributing more false positives to women and more false
negatives to men, and in order to rectify this imbalance, we would need to use
a different threshold of qualification for women than for men, sacrificing the
accuracy of the model.

A second concern with equalized odds is that it is easily manipulable. Consider
the following example from criminal justice. We have a distribution rule which
says to allocate a parole to a prisoner if they are very unlikely to recidivate.
Due to a history of discriminatory practices and social marginalization, black
prisoners are judged as having higher a risk of recidivism much higher than
white defendants~\citep{CrimeJustice_2023}. As a result, allocating a parole to 
a white prisoner has a lower likelihood of being a false positive. Therefore,
one could achieve equal false positive rates by \textit{adding} false
positives to the white portion of the dataset, resulting in an increase in the
number of undeserving white prisoners receiving parole.

As an example of equalized odds gone wrong, consider the COMPAS
algorithm~\citep{Angwin_2016}. COMPAS was calibrated to have equal predictive
accuracy across racial groups, but this resulted in a higher false positive rate
for black defendants and a much lower false positive rate for white defendants
due to unequal base rates.

\begin{definition}
    Counterfactual Fairness — $f$ satisfies counterfactual fairness iff
    \[P[f_{A \leftarrow a}(X) = 1 | X = x, A = a] = P[f_{A\leftarrow b}(X) = 1 |
         X = x, A = a]\;\forall a, b \in A\]~\citep{Kusner_2018}.
    Where $P(f_{A \leftarrow a})$ is the counterfactual value of $f$ if $A$ were
    set to $a$.
\end{definition}

Borrowing from the language of causal inference, counterfactual fairness posits
that the protected attributes may not have any causal effect on the outcome of
the classification function. This measure operates on the individual
counterfactual—would the output of the classification function have been 
different if the individual had been different according to some protected
attribute? This is a highly appealing notion of fairness.
If their protected characteristics do not in any way cause their outcome, then
it is difficult to argue that one has been discriminated against. However,
this measure is difficult to implement in practice.

Counterfactual fairness is often critiqued based on the difficulty and potential
subjectivity of detecting causal links between variables. Recent work on the 
social construction of demographic variables reveals that causal modeling may 
have an inherently normative basis~\citep{Hu_Forthcoming}, and even if these
issues are set aside, the computational expense of causal discovery can create
issues of practicality.

This discussion of dominant algorithmic fairness measures and their critiques
reveal that there is no one-size-fits-all solution to the problem of algorithmic
fairness. Each measure has its own strengths and weaknesses, and the choice of
measure will depend on the specific context in which the algorithm is being
applied. Intuitively, one would like to measure a given classifier against a
range of measures to ensure it is fair in a variety of ways. However, as
previously mentioned, it is impossible to satisfy multiple measures
simultaneously, and so the choice of measure becomes a critical one.
How should one select a measure? What are the philosophical
considerations that should guide this choice? In hopes of developing a more
nuanced and structured approach to these questions, we turn to work in the 
philosophy of distributive justice.

\subsection{Theories of Distributive Justice}

Distributive justice is a philosophical field of inquiry that examines how to
define a fair allocation of goods and resources across a society. A fully
fledged account of distributive justice must answer a number of questions. Who
should receive those resources which are highly scarce? When and why is it 
allowed for one person to have more of something than another? By what mechanism
can resources be redistributed to achieve justice?

Given that distributive justice defines how fair decisions about allocations can
be made, within the formalism we've presented, its role is to broadly define the
decision rule which may then be implemented algorithmically. As described in
section~\ref{sec:introduction}, the dominant theory of distributive justice used
in connection with algorithmic decision making is John Rawls' theory of liberal
egalitarianism, which we will present here.

Rawls introduces his account of justice through a thought-experiment called the
veil of ignorance. In this thought-experiment, one is asked to imagine
themselves in a pre-societal world, working in collaboration with a number of
others to determine how resources should be allocated across society once it
begins. Critically, all those involved in designing this distribution of goods
are unaware of what their own position and endowments in society will be. One
may find themselves endowed with a high level of intelligence, or a valuable
skill, or wealth at birth, or one may find themselves with none of these things,
or the opposite. Without knowing which of these positions one will occupy,
Rawls argues that one will be motivated to design a society in which the
following two principles are satisfied:

\begin{enumerate}
    \item Each person has an indefeasible right to the most extensive basic 
          liberties compatible with equal liberty for all.
    \item Social and economic inequalities are to be arranged so that they are
          both to the greatest benefit of the least advantaged, to offices
          open under fair equality of opportunity. (dubbed the
          \textit{difference principle}).
\end{enumerate}

Rawls refers to the group of individuals designing the society from behind the
veil of ignorance as the \textit{original position}. The argument from the
original position results in citizens living under a social contract which
is guided by the two principles given above. The principles allow us to measure,
for any given distribution of resources across society, whether or not the
distribution is fair. If the distribution is not fair, then Rawls endorses
a program of redistribution to bring the distribution into line with the
principles. For example, a society with a high-level of wealth inequality is
in violation of the difference principle — the wealth gap represents an economic
inequality which is not to the greatest benefit of the least advantaged. In this
case, Rawls would endorse a program of redistribution to balance the wealth
across the society in accord with the principles given above.

This type of distributive justice theory is what we refer to as an
\textit{end-state} theory of justice. The distribution of goods across society
represents a discretely evolving state of affairs, and the role of the theory is
to determine whether or not each state is just. Let us consider this view in
light of the decision problem we formalized above. Liberal egalitarianism tells
us that the decision rule $d$ must be such that either all individuals receive
the resource of allocation equally, or that inequalities in the allocation of
resources must be to the benefit of the least advantaged. Several of the
fairness criteria in the literature on algorithmic fairness can be seen as
implementing the first condition in terms of equality of opportunity —— by 
regulating the extent and manner in which protected attributes can influence the
outcome of the decision, we attempt to ensure that all individuals are receive
equal basic rights of opportunity in the decision process. However, whether
or not the difference principle is satisfied by these measures is less clear.

In our loan case, for example, we may be concerned with ensuring that every
individual receives equal opportunity to a loan. However, if the decision rule
is such that only individuals with a household income above \$100,000 per year,
or those who are members of a particular race, are able to receive a loan, then
this clearly doesn't provide equal opportunity to all. A measure like
demographic parity ensures that individuals from each protected group are equally
likely to receive a loan, therefore balancing opportunity across groups. However,
whether or not this satisfies the difference principle depends on the base rates
of success across groups. If the base rate of loan default is higher for men
than for women, demographic parity may require that we give loans to some men
who are less likely to pay back their loans rather than qualified women. Absent
from this metric is any consideration of who is considered to be the least
advantaged in the situation. By Rawls, we should be allocating capital to those
individuals who have the least access to capital, a condition which is not
satisfied by demographic parity. \citep{Hertweck_2023} shows how Rawls' theory
can be more fully captured in a group-based fairness metric, which we will not
go through the details of here. However, the key point echoed throughout the
literature is that the common measures of algorithmic fairness stem from 
egalitarianism, and it is unclear how to extend these measures to a theory like Nozick's
which does not operate on an end-state theory of justice.
