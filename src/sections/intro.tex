%% INTRO SECTION  %%

The rise of algorithmic decision making in the public sector has caused
significant public concer. As algorithms increasingly make decisions that affect
individials' lives, from determining creditworthiness to predicting criminal 
recidivism, the public has grown fearful of their potential to perpetuate 
existing social inequalities. A 2018 study showed that 58\% of Americans
believe that algorithms will always have some level of bias~\cite{Smith_2018}, 
and as we know from the famed COMPAS case, these fears are not
unfounded~\cite{Angwin_2016}.

In response to these concerns, researchers have developed two broad and
increasingly vast bodies of work. The first, which we will refer to as
\textit{algorithmic fairness}, focuses on developing statistical and
computational tools to ensure that algorithms do not discriminate against
protected groups. The second, referred to as \textit{algorithmic
accountability}, focuses on explainability and interpretability â€” developing
tools to help users understand and interpret the decisions made by algorithms. 
The former area of research is what we will focus on in this paper.

The field of algorithmic fariness is often conceptualized as the application of
the philosophical notion of distributive justice to algorithmic decision
making. At first glance, this seems like a natural fit. The goal of distributive
justice is to ensure that the allocation of the benefits and burdens of society
are distributed fairly among its members, and the goal of algorithmic fairness
measures are to ensure that the allocation of decisions by algorithms complies
with some notion of fariness. However, recent work questions this
analogy~\cite{Hertweck_2024}, analytically showing that the extent to which 
algorithmic fairness measures can be seen as a form of distributive justice
is quite limited, and isolated to egalitarian concepts of
justice~\cite{Kuppler_2021}.

In this paper, we propose a new direction for research that incorporates a 
previously overlooked distributive justice concept: entitlement justice.
Entitlement theory, which roots justice in the idea of respecting individuals'
property rights, offers a more nuances and context-sensitive understanding of
algorithmic fariness. We argue that by incorporating entitlement justice into
the design of algorithmic fairness measures, we can create a more robust
framework for evaluating algorithmic decisions. When this framework is applied
to the broader sociotechnical systems in which algorithms are embedded, we can
better understand the social implications of algorithmic decision making and
develop more effective strategies for mitigating their negative effects.

The rest of this paper is organized as follows. In Section~\ref{sec:background},
we provide an overview of the existing literature on algorithmic fairness and
distributive justice. We draw on the formalism from~\cite{Kuppler_2021}
and~\cite{CorbettDavies_2023} to create a unified model for understanding
algorithmic fairness and distributive justice consistently with each other. 
In Section~\ref{sec:entitlement-justice}, we introduce the concept of
entitlement justice and discuss its historical development. We confront the 
traditional objections to entitlement theory and show how they can be overcome
in the context of algorithmic decision making. In
Section~\ref{sec:entitlement-fairness}, we propose a new framework for
understanding algorithmic fairness through the lens of entitlement justice. We
analyze the implications of this framework for existing algorithmic fairness
measures and show an example of how it can be applied to a real-world case
study. Finally, in Section~\ref{sec:conclusion}, we conclude with a discussion
of the broader implications of our work and suggest directions for future
research.